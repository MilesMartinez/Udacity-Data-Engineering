# Purpose
This database enables Sparkify to easily query the data they collect from the users of their music streaming service for analysis. Before establishing the database, the data was previously available only in its raw form as JSON logs stored on the app. By aggregating all the user data into a centralized database in an organized format, Sparkify can easily conduct queries for their analysis. In this case, they are interested in understanding what songs users are listening to.
# Design
## Schema

There is one centralized table of interest, the *songplays* table, which references data from the following 4 other tables:
* *songs*
* *users*
* *artists*
* *time*

The *songplays* table references the values in the other tables via a unique ID. We use IDs, and not a direct referrence (i.e. the actual name of the artist, user, or song). This is so if were to ever want to update an artist name for example, we do not have to update every single row in the *songplays* and the *songs* table with the new artist name. We simply update ONE name in the *artists* table and the reference ID of the artist stays the same in every row of the other tables.

## ETL Pipeline
The ETL pipeline conducts the following 3 things to build the database:
1. Extracts the user data from the files generated by the music streaming app
2. Transforms the data from ugly JSON into nicely structured tables
3. Builds the database and loads the data into it for the analytics team to use

# Files
* the **data** directory contains all the user, song, and log data.
* **create_tables.py** creates the database and tables
* **sql_queries.py** contains all PostgreSQL querys stored as python variables so they can be used by etl.py 
* **etl.py** runs the ETL pipline which processes the data in the **data** directory


# How to Run
First create the database and tables.
```sh
$ python create_tables.py
```
Then execute the ETL process.
```sh
$ python etl.py
```
**Woohoo! The database is now created and ready for use!**
