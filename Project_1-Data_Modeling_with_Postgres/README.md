# Purpose
This database enables Sparkify to easily query the data they collect from the users of their music streaming service for analysis. Before establishing the database, the data was previously available only in its raw form as JSON logs stored on the app. By aggregating all the user data into a centralized database in an organized format, Sparkify can easily conduct queries for their analysis. In this case, they are interested in understanding what songs users are listening to.
# Design
### Schema
The table of interest is the *songplays* dimension table, which leverages data from the following 4 fact other tables:
* *songs*
* *users*
* *artists*
* *time*

The *songplays* table referes to some of these values by a unique ID. We use IDs, and not a direct referrence (i.e. the actual name of the artist, user, or song) so that if were to ever want to change, for example, an artist name, we do not have to update ever single row in the *songplays* and the *songs* table with the new artist name. We simply update one name in the *artists* table and the ID of the artist stays the same.


### ETL Pipeline
The ETL pipeline conducts the following 3 things to build the database:
1. Extracts the user data from the files generated by the music streaming app
2. Transforms the data from ugly JSON into nicely structured tables
3. Builds the database and loads the data into it for the analytics team to use

# Files
* the **data** directory contains all the user, song, and log data.
* **create_tables.py** creates the database and tables
* **sql_queries.py** contains all PostgreSQL querys stored as python variables so they can be used by etl.py 
* **etl.py** runs the ETL pipline which processes the data in the **data** directory


# How to Run
First create the database and tables.
```sh
$ python create_tables.py
```
Then execute the ETL process.
```sh
$ python etl.py
```
**Woohoo! The database is now created and ready for use!**
