# Project Summary
This data lake enables Sparkify to easily query the data they collect from the users of their music streaming service for analysis. Before establishing the data lake, the data was previously available only in its raw form as JSON logs generated by user activity on the app and stored in S3. By creating an ETL pipline that transfers the data to a Spark Cluster where it is processed and formatted into a form that is easy to query, Sparkify can easily use the data for analysis. We choose to use a data lake, rather than a data wharehouse, because Sparkify has grown their user base substantially where it would be benefial to leverage big data tools, such as Spark, to process the data with parallization.

# Design
## Schema
Fact Table:
* *songplays*

Dimension Tables:
* *songs*
* *users*
* *artists*
* *time*

The *songplays* table referes to some of its attributes in the dimension tables by a unique ID. By using these IDs, we treat the dimension tables as lookup tables to give us more information on some of the attributes in the *songplays* table. We do not use direct referrences (i.e. the actual name of the artist, user, or song) in the fact table so that if were to ever want to change, for example, an artist name, we do not have to update every single row in both *songplays* and the *songs* table with the new artist name. We simply update one name in the *artists* table and the ID of the artist stays the same.


## ETL Pipeline
The ETL pipeline conducts the following 3 things to build the database:
1. **Extracts** the user data from the files generated by the music streaming app, which is held in an S3 bucket
2. Processes the data using Spark to **transform** it from ugly JSON into nicely structured tables for the analytics team to leverage
3. **Loads** the newly created tables back into S3 as .parquet files

## Files
* **etl.py** runs the ETL pipline which processes the song and event data in Spark
* **dl.cfg** is the config file containing all the variables needed for connecting to specified AWS resources


# How to Run
To execute the ETL process.
```sh
$ python etl.py
```