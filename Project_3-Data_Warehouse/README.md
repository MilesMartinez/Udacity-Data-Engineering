# Project Summary
This database enables Sparkify to easily query the data they collect from the users of their music streaming service for analysis. Before establishing the database, the data was previously available only in its raw form as JSON logs stored on the app. By creating an ETL pipline that transfers the data to a Redshift Cluster and formats it in a usable form, Sparkify can easily conduct queries for their analysis.

# Design
## Schema

Fact Table:
* *songplays*

Dimension Tables:
* *songs*
* *users*
* *artists*
* *time*

The *songplays* table referes to some of its attributes in the dimension tables by a unique ID. By using these IDs, we treat the dimension tables as lookup tables to give us more information on some of the attributes in the *songplays* table. We do not use direct referrences (i.e. the actual name of the artist, user, or song) in the fact table so that if were to ever want to change, for example, an artist name, we do not have to update ever single row in both *songplays* and the *songs* table with the new artist name. We simply update one name in the *artists* table and the ID of the artist stays the same.


## ETL Pipeline
The ETL pipeline conducts the following 3 things to build the database:
1. Extracts the user data from the files generated by the music streaming app, which is held in an S3 bucket.
2. Transforms the data from ugly JSON into nicely structured tables
3. Builds the database in an Amazon Redshift cluster and loads the data into it for the analytics team to use

## Files
* **create_tables.py** creates the database and tables in an Amazon Redshift cluster
* **sql_queries.py** contains all Redshift SQL querys stored as python variables so they can be used by etl.py 
* **etl.py** runs the ETL pipline which processes the song and event data in S3
* **dwh.cfg** is the config file containing all the variables needed for connecting to specified AWS resources


## How to Run
First create the database and tables.
```sh
$ python create_tables.py
```
Then execute the ETL process.
```sh
$ python etl.py
```
**Woohoo! The database is now created and ready for use!**
